{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Classification Task using BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "import pickle\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.utils import to_categorical\n",
    "from keras import models, optimizers, preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, Embedding, SimpleRNN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras import models, optimizers, Input\n",
    "import numpy as np\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from keras import models, optimizers, Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Embedding, concatenate, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=100)\n",
    "import os \n",
    "imdb_dir = './aclImdb/' \n",
    "train_dir = os.path.join(imdb_dir, 'train') \n",
    "labels = []\n",
    "texts = [] \n",
    "count_pos = 0\n",
    "count_neg = 0\n",
    "\n",
    "for label_type in ['neg', 'pos']: \n",
    "    dir_name = os.path.join(train_dir, label_type) \n",
    "    for fname in os.listdir(dir_name): \n",
    "        if fname[-4:] == '.txt': \n",
    "            f = open(os.path.join(dir_name, fname)) \n",
    "            texts.append(f.read())\n",
    "            f.close() \n",
    "            if label_type == 'neg': \n",
    "                labels.append(0)\n",
    "            else: \n",
    "                labels.append(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bert server by following guidelines here\n",
    "\n",
    "https://github.com/hanxiao/bert-as-service\n",
    "\n",
    "or to obtain elmo-like embeddings you can server using : \n",
    "```\n",
    "bert-serving-start -pooling_strategy NONE -max_seq_len 256 -model_dir multi_cased_L-12_H-768_A-12\n",
    "\n",
    "```\n",
    "multi_cased_L-12_H-768_A-12 : this is model download from bert repository. you can download other models and try those also.\n",
    "\n",
    "\n",
    "you have to install bert-server and bert client using :\n",
    "\n",
    "```\n",
    "pip install bert-serving-server  # server\n",
    "pip install bert-serving-client  # client, independent of `bert-serving-server`\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data\n",
    "bc = BertClient(check_length=False)\n",
    "claims_vec = bc.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "claims (InputLayer)          (None, 138, 768)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 16)                50240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 50,385\n",
      "Trainable params: 50,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 379s 17ms/step - loss: 0.4838 - acc: 0.7680 - val_loss: 0.6870 - val_acc: 0.6892\n",
      " — val_f1: 0.804923 — val_precision: 0.926555 — val_recall 0.711520\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 378s 17ms/step - loss: 0.4110 - acc: 0.8286 - val_loss: 0.3716 - val_acc: 0.8356\n",
      " — val_f1: 0.856133 — val_precision: 0.859412 — val_recall 0.852880\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 381s 17ms/step - loss: 0.3602 - acc: 0.8489 - val_loss: 0.4706 - val_acc: 0.7928\n",
      " — val_f1: 0.851967 — val_precision: 0.899804 — val_recall 0.808960\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 379s 17ms/step - loss: 0.3337 - acc: 0.8621 - val_loss: 0.4293 - val_acc: 0.8144\n",
      " — val_f1: 0.866887 — val_precision: 0.894553 — val_recall 0.840880\n"
     ]
    }
   ],
   "source": [
    "class Metrics(Callback):\n",
    "\n",
    "\tdef on_train_begin(self, logs={}):\n",
    "\n",
    "\t\tself.val_f1s = []\n",
    "\t\tself.val_recalls = []\n",
    "\t\tself.val_precisions = []\n",
    "\t \n",
    "\tdef on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "\t\tval_predict = (np.asarray(model.predict([claims_vec]))).round()\n",
    "\t\tval_targ = labels\n",
    "\t\t_val_f1 = f1_score(val_targ, val_predict, average = 'binary')\n",
    "\t\t_val_recall = recall_score(val_targ, val_predict, average ='binary')\n",
    "\t\t_val_precision = precision_score(val_targ, val_predict,average = 'binary')\n",
    "\t\tself.val_f1s.append(_val_f1)\n",
    "\t\tself.val_recalls.append(_val_recall)\n",
    "\t\tself.val_precisions.append(_val_precision)\n",
    "\t\tprint (' — val_f1: %f — val_precision: %f — val_recall %f' %( _val_f1, _val_precision, _val_recall))\n",
    "\t\treturn\n",
    "\n",
    "\n",
    "\n",
    "class TrainModel:\n",
    "\n",
    "    def lstm_model(self, claim_length, embedding_dim, nb_classes):\n",
    "\n",
    "\n",
    "            claims_input = Input(shape=(claim_length, embedding_dim), dtype='float32', name='claims')\n",
    "            encoded_claims = LSTM(32, return_sequences=True, recurrent_dropout=0.4)(claims_input)\n",
    "            encoded_claims = LSTM(16, recurrent_dropout=0.4)(claims_input)\n",
    "\n",
    "            concatenate_layers = Dense(8, kernel_regularizer=regularizers.l2(0.001), activation='relu')(encoded_claims)\n",
    "            # concatenate_layers = Dropout(0.6)(concatenate_layers)\n",
    "            # concatenate_layers = Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu')(concatenate_layers)\n",
    "            # concatenate_layers = Dense(32, kernel_regularizer=regularizers.l2(0.001), activation='relu')(concatenate_layers)\n",
    "\n",
    "            # concatenate_layers = Dropout(0.6)(concatenate_layers)\n",
    "            if nb_classes == 3:\n",
    "\n",
    "                pred_label = Dense(3, activation='softmax')(concatenate_layers)\n",
    "\n",
    "            else:\n",
    "\n",
    "                pred_label = Dense(1, activation='sigmoid')(concatenate_layers)\n",
    "\n",
    "\n",
    "\n",
    "            return claims_input, pred_label\n",
    "    \n",
    "    \n",
    "training = TrainModel()\n",
    "metrics = Metrics()\n",
    "\n",
    "max_claims_length = 138\n",
    "max_sents_length = 138\n",
    "\n",
    "nb_classes = 1\n",
    "\n",
    "if nb_classes == 3:\n",
    "    labels = np_utils.to_categorical(labels, nb_classes)\n",
    "    loss = 'categorical_crossentropy'\n",
    "\n",
    "else:\n",
    "    loss = 'binary_crossentropy'\n",
    "\n",
    "embedding_dim = 768\n",
    "claims_input, pred_label = training.lstm_model(max_claims_length, embedding_dim, nb_classes)\n",
    "\n",
    "\n",
    "model = Model([claims_input], pred_label)\n",
    "print (model.summary())\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "model.compile(optimizer=optimizers.Adam(), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit({'claims': claims_vec}, labels, \n",
    "                            epochs=5, batch_size=32, validation_split=0.1, callbacks=[early_stopping, metrics,\n",
    "                                            ModelCheckpoint(filepath='model_imdb_lstm_bert.h5', monitor='val_loss', save_best_only=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
