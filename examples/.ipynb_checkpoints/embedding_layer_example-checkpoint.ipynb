{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Embedding layer provided by Keras for deep learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEmbedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\\n    i. input dim:  It is size of the vocabulary in the text data. \\n        For example, if your data is integer encoded to values between 0-6, \\n        then the size of the vocabulary would be 7 words.\\n    ii. output dim: It is the size of the vector space in which words will be embedded.\\n        It defines the size of the output vectors from this layer for each word. \\n        For example, it could be 32 or 100 or even larger. Test different values for your problem.\\n    iii. input_length: This is the length of input sequences, as you would define \\n        for any input layer of a Keras model.\\n        For example, if all of your input documents are comprised of 1000 words, this would be 1000.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use Keras embedding layer to convert text into numerical value for neural network training\n",
    "# It requires that the input data be integer encoded, so that each word is represented by a unique integer\n",
    "# Embedding layer is initialized with random weights and and will \n",
    "#learn an embedding for all of the words in the training dataset.\n",
    "'''\n",
    "Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "    i. input dim:  It is size of the vocabulary in the text data. \n",
    "        For example, if your data is integer encoded to values between 0-6, \n",
    "        then the size of the vocabulary would be 7 words.\n",
    "    ii. output dim: It is the size of the vector space in which words will be embedded.\n",
    "        It defines the size of the output vectors from this layer for each word. \n",
    "        For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "    iii. input_length: This is the length of input sequences, as you would define \n",
    "        for any input layer of a Keras model.\n",
    "        For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25, 48], [17, 22], [28, 40], [18, 22], [34, 48], [3, 22], [3, 28], [37, 3], [48, 40], [40, 19, 25, 35, 16]]\n"
     ]
    }
   ],
   "source": [
    "# Define documents \n",
    "# 5 positive and 5 negative samples\n",
    "docs =  [\"well done\", \"nice work\", \n",
    "         \"excellent job\", \"good work\", \n",
    "         \"nicely done\", \"poor work\", \"very bad\", \n",
    "         \"weak effort\", \"Improper job\", \n",
    "        \"Could have been improved more\"]\n",
    "labels = [1,1,1,1,1,0,0,0,0,0]\n",
    "\n",
    "# We encode all the docs into integers\n",
    "# we use one_hot function provided by keras,\n",
    "# it assigns unique number to each word\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25 48  0  0]\n",
      " [17 22  0  0]\n",
      " [28 40  0  0]\n",
      " [18 22  0  0]\n",
      " [34 48  0  0]\n",
      " [ 3 22  0  0]\n",
      " [ 3 28  0  0]\n",
      " [37  3  0  0]\n",
      " [48 40  0  0]\n",
      " [19 25 35 16]]\n"
     ]
    }
   ],
   "source": [
    "# According to Keras, all the docs should have same length\n",
    "# since in our doc, one doc has more words(4) so we need to use padding \n",
    "# pad docs to maximum length of 4\n",
    "pad_max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=pad_max_length, padding='post')\n",
    "print padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# Define the model \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=pad_max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print model.summary()\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
